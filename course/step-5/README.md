## Шаг 5: Подключение Qdrant, векторизация и индексация

### Цель:
Научиться работать с векторными базами данных, используя **Qdrant** для индексирования и поиска векторов, полученных из текстовых данных. Мы будем выполнять несколько шагов: от регистрации и активации базы через API-ключ до загрузки и индексации данных в базе после их обработки и векторизации.

![image](https://github.com/user-attachments/assets/eda6094f-5ddb-45b9-aa4b-c1966f22cf35)

## **Sentence-BERT** и его применение в задаче

**Sentence-BERT** и **BERT** — это два варианта модели на базе трансформеров, которые используются для обработки естественного языка, но у них есть ключевые различия, касающиеся задач, для которых они оптимизированы.

### 1. **BERT** (Bidirectional Encoder Representations from Transformers)

- **Цель**: как уже было до этого сказано, BERT — это модель для обучения представлений текста (эмбеддингов), которая обучена на больших объемах текста с помощью задачи Masked Language Model (MLM) и Next Sentence Prediction (NSP). Модель обучается, чтобы понимать контекст слов в предложении, обращая внимание как на предшествующий, так и на последующий текст (два направления).

### 2. **Sentence-BERT** (Sentence-BERT)

- **Цель**: **Sentence-BERT** (или **SBERT**) — это модификация BERT, оптимизированная для задач, где нужно работать с **предложениями или текстами целиком**, а не с отдельными словами. Эта модель специально разработана для создания **эмбеддингов предложений** и эффективного их сравнения.

- **Применение**: Sentence-BERT идеально подходит для задач, которые включают:
  - **Поиск** и **сравнение предложений** (например, поиск похожих предложений, кластеризация текста, семантическое сравнение).
  - **Сопоставление вопросов и ответов** (например, для автоматического ответа на вопросы или в системах вопрос-ответ).
  - **Задачи семантического поиска** (например, использование эмбеддингов для улучшения поиска по документам).
  
  SBERT использует модификацию BERT для более эффективного создания фиксированных эмбеддингов предложений, что делает её быстрее и точнее для этих задач.

- **Особенность**: SBERT генерирует **фиксированные эмбеддинги для целых предложений**. Это значит, что вместо того, чтобы генерировать эмбеддинги для отдельных слов, модель выдает одно векторное представление для всего предложения, которое можно использовать для сравнения с другими предложениями.

### Основные различия:

1. **Предназначение**:
   - **BERT** фокусируется на обработке и извлечении представлений для отдельных токенов или слов в тексте.
   - **Sentence-BERT** оптимизирован для получения эмбеддингов целых предложений и их дальнейшего использования в задачах сравнения.

2. **Преобразование эмбеддингов**:
   - **BERT** генерирует эмбеддинги для каждого слова (или токена) в предложении.
   - **Sentence-BERT** генерирует одно фиксированное представление (вектор) для всего предложения, что делает её более эффективной для задач сравнения и поиска.

3. **Оптимизация**:
   - **Sentence-BERT** использует модификацию BERT, добавляя дополнительные слои, которые обеспечивают эффективное извлечение эмбеддингов предложений с минимальными вычислительными затратами, что ускоряет задачи сравнения и поиска.
   
4. **Задачи**:
   - **BERT** используется для широкого круга задач, включая классификацию, извлечение информации и другие, где важно понимать контекст слов в предложении.
   - **Sentence-BERT** идеально подходит для задач, где нужно сравнивать или искать схожие предложения, такие как поиск похожих вопросов или определение схожести между предложениями.

### Пример:

- **BERT**: для каждого слова в предложении модель генерирует эмбеддинг, например:
  - "Как зарегистрироваться на курс?" — эмбеддинг для "Как", эмбеддинг для "зарегистрироваться", и так далее.

- **Sentence-BERT**: для этого же предложения модель генерирует одно векторное представление, которое отражает смысл всего предложения, что позволяет быстро сравнивать это предложение с другими предложениями, например, в задаче семантического поиска.

### Заключение:

- **BERT** — это мощная модель для генерации контекстных эмбеддингов слов и решения задач NLP, требующих понимания структуры текста.
- **Sentence-BERT** — это улучшенная версия для задач, связанных с **сравнением предложений** и их эффективным поиском, особенно в zero-shot задачах, таких как семантический поиск, классификация текстов или выявление схожести между текстами.

Таким образом, **Sentence-BERT** является более подходящей моделью, если ваша задача включает работу с предложениями или текстами целиком, особенно если нужно сравнивать их или искать семантические связи между ними.

### **Sentence-BERT** и его применении в задаче

**Sentence-BERT (SBERT)** — это модификация модели **BERT**, разработанная для создания **векторных представлений предложений**. В отличие от оригинального BERT, который фокусируется на обработке отдельных слов и их взаимосвязей, Sentence-BERT оптимизирован для того, чтобы генерировать плотные векторы для целых предложений или текстов. Это позволяет эффективно работать с задачами, связанными с **поиском по сходству** и **кластеризацией текстов**, такими как наша задача с индексацией данных в **Qdrant**.

![image](https://github.com/user-attachments/assets/ea0dd749-ac1c-43fd-b2fa-da316db171c5)

### Преимущества использования **Sentence-BERT** для нашей задачи:

1. **Эффективность векторизации предложений**: Sentence-BERT оптимизирован для того, чтобы преобразовывать целые предложения или абзацы в векторные представления, что идеально подходит для работы с текстами курсов (например, описаниями, вопросами и ответами). Это позволяет эффективно сравнивать тексты и искать похожие фрагменты.

2. **Скорость и точность**: Благодаря использованию **вычислительных оптимизаций**, таких как **двойное внимание** и **обучение с использованием пар данных**, Sentence-BERT может эффективно работать с большими объемами текстов, что важно для нашей задачи, где требуется векторизовать множество текстов и загрузить их в базу данных для дальнейшего поиска.

3. **Семантическое сходство**: Модели, такие как **Sentence-BERT**, позволяют искать тексты, которые по смыслу близки друг другу, а не только по точному совпадению слов. Это особенно полезно для задач, где необходимо найти схожие вопросы и ответы в FAQ или описания курсов.

### Интеграция с **Qdrant**:
- **Qdrant** — это векторная база данных, предназначенная для хранения и поиска векторов. После векторизации текста с помощью **Sentence-BERT**, эмбеддинги могут быть загружены в **Qdrant**, что позволяет выполнять быстрые запросы для поиска похожих текстов (например, поиск схожих описаний курсов или ответов на вопросы).
- Благодаря возможности индексировать векторы и эффективно искать ближайших соседей, **Qdrant** идеально подходит для использования с **Sentence-BERT**, чтобы обеспечить эффективный и быстрый поиск по большим объемам текстовых данных.
- Если у вас ещё нет кластера в Qdrant, то перейдите на [официальный сайт](https://qdrant.tech/) и бесплатно зарегистрируйтесь (авторы предлагают в пользование один кластер размером до 1 GB).

**Итог**: Использование **Sentence-BERT** для векторизации текста и **Qdrant** для хранения и поиска эмбеддингов предоставляет мощную и эффективную систему для работы с текстами и их быстрого поиска на основе семантического сходства.

![image](https://github.com/user-attachments/assets/b2e48f51-4c31-4775-83be-b3992c839cb2)

### Введение
- Сначала напишите переменную `qdrant_client` (инициализация существующего кластера Qdrant)

### Задание 1 - Функция для векторизации FAQ:
   - Напишите функцию `create_faq_embeddings()`, которая будет принимать список чанков FAQ и возвращать их векторные представления (эмбеддинги).
   - Используйте модель **Sentence-BERT** для преобразования каждого чанка в вектор.
   - Для оптимизации памяти, реализуйте **батчинг** (batching), чтобы обрабатывать данные порциями, а не загружать всё сразу в память.

### Задание 2 - Загрузка FAQ в Qdrant:
   - Напишите функцию `upload_faq_to_qdrant()`, которая будет загружать векторизованные FAQ в коллекцию **Qdrant**.
   - Если коллекция с таким именем уже существует, удалите её и создайте новую.
   - Каждый чанк должен быть добавлен в коллекцию с соответствующими метаданными, такими как `chunk_id`, `course_name`, `question`, `answer` и другие.

### Задание 3 - Функция для векторизации основной информации о курсе:
   - Напишите функцию `create_info_embeddings()`, которая будет векторизовать чанки основной информации о курсе (например, описание курса).
   - Используйте **Sentence-BERT** и батчинг для обработки чанков.

### Задание 4 - Загрузка основной информации о курсе в Qdrant:
   - Напишите функцию `upload_info_to_qdrant()`, которая будет загружать векторизованные данные основной информации о курсе в Qdrant.
   - Убедитесь, что в каждом чанк добавлены все метаданные, такие как `chunk_id`, `course_name`, `sequence`, `categories` и другие.

---

### Подсказки для реализации:

- Для векторизации используйте библиотеку **sentence-transformers** и модель `DeepPavlov/rubert-base-cased-sentence`.
- Для работы с базой данных **Qdrant** используйте клиентскую библиотеку `qdrant-client`. Убедитесь, что вы создаете коллекцию с правильными параметрами вектора и метаданными.
- Для батчинга (обработки данных порциями) используйте цикл, который будет разбивать данные на более мелкие части и векторизовать их по частям.

### Ожидаемый результат:
- Векторизованные данные (FAQ и информация о курсах) будут загружены в **Qdrant** и готовы к поисковым операциям.
- Эмбеддинги должны быть созданы эффективно с использованием батчинга для оптимизации работы с памятью

---

### Что проверяется:
- Правильность разбития на структурированные фрагменты текста и загрузки в векторную базу с помощью двух функций: `upload_faq_to_qdrant()` и `upload_info_to_qdrant()`.

### Как сдавать задание:
- Обязательно определите переменную с данными своей векторной базой Qdrant (это нужно для проверки правильности работоспособности базы):

```python
qdrant_client = QdrantClient(
    url="https://###############.cloud.qdrant.io",
    api_key="################"
)
```

- Заранее определить импорты нужных библиотек (например, для аннотаций и работы с Qdrant).
- Загрузить скрипты с функциями `upload_faq_to_qdrant()` и `upload_info_to_qdrant()`.

**Примечание**: при проверке заданий эмбеддинги берутся случайные - размерностью 512, чтобы сэкономить время на проверку. Поэтому векторизация в функциях `create_faq_embeddings()` и `create_info_embeddings` тоже не проверяется.

## Практическая часть (шаблоны для кода)

### Задание 1 - Функция для векторизации FAQ

```python
from typing import List, Dict
import pandas as pd
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from qdrant_client.models import PointStruct
from sentence_transformers import SentenceTransformer


# загрузка модели для векторизации
model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence').to(device)

# пример для уточнения длины вектора (требуется при создании коллекции в кластере Qdrant)
сourse_vector_length = model.encode(faq_collection['https://karpov.courses/dataengineer-start'][0], show_progress_bar=False).shape[0]

def create_faq_embeddings(chunks: List[Dict], batch_size: int = 32) -> np.ndarray:
    """
    Создает эмбеддинги для чанков с батчингом для оптимизации памяти.

    Эта функция преобразует список чанков (вопросов и ответов) в эмбеддинги с использованием модели. 
    Для оптимизации использования памяти эмбеддинги создаются пакетами (batching).

    Параметры:
    - chunks (List[Dict]): Список чанков, каждый из которых должен содержать информацию о вопросе и ответе.
    - batch_size (int): Размер пакета для обработки. По умолчанию равен 32.

    Возвращаемое значение:
    - np.ndarray: Массив эмбеддингов для каждого чанка.
    """
    texts = ...
    
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = ...
        embeddings.extend(batch_embeddings)

        print(f'{i} of {len(texts)} FAQ is done!')
    
    embeddings = np.array(embeddings)
    return embeddings
```

### Задание 2 - Загрузка FAQ в Qdrant

```python
def upload_faq_to_qdrant(chunks: List[Dict], embeddings: np.ndarray, collection_name: str = 'karpov-guide-faq'):
    """
    Загружает данные в Qdrant с оптимизированными параметрами.

    Эта функция загружает чанки текста и их эмбеддинги в коллекцию Qdrant. 
    Если коллекция уже существует, она будет удалена и создана заново.

    Параметры:
    - chunks (List[Dict]): Список чанков, каждый из которых содержит текстовые данные (вопрос, ответ и метаданные).
    - embeddings (np.ndarray): Эмбеддинги для чанков, полученные после векторизации текста.
    - collection_name (str): Название коллекции в Qdrant. По умолчанию 'karpov-guide-faq'.

    Возвращаемое значение:
    - None
    """
    if qdrant_client.collection_exists(collection_name):
        qdrant_client.delete_collection(collection_name)  # Удаляем, если уже создана
            
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=..., distance=Distance.COSINE),
    )
    
    points = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        point = PointStruct(
            id=i,
            vector=embedding.tolist(),
            payload={
                'chunk_id': ...,
                'course_name': ...,
                'course_url': ...
                'question': ...,
                'answer': ...,
                'metadata': ...
            }
        )
        points.append(point)
    
    qdrant_client.upload_points(
        collection_name=collection_name,
        points=points,
        batch_size=30
    )
```

### Задание 3 - Функция для векторизации основной информации о курсе

```python
def create_info_embeddings(chunks: List[Dict], batch_size: int = 32) -> np.ndarray:
    """
    Создает эмбеддинги для чанков с батчингом для оптимизации памяти.

    Эта функция преобразует список чанков информации о курсах в эмбеддинги с использованием модели. 
    Эмбеддинги создаются пакетами для оптимизации использования памяти.

    Параметры:
    - chunks (List[Dict]): Список чанков с информацией о курсах, где каждый чанк содержит описание и категории.
    - batch_size (int): Размер пакета для обработки. По умолчанию равен 32.

    Возвращаемое значение:
    - np.ndarray: Массив эмбеддингов для каждого чанка.
    """
    embeddings = []
    for i in range(0, len(chunks), batch_size):
        batch_texts = ...
        batch_embeddings = ...
        embeddings.extend(batch_embeddings)

        print(f'{i} of {len(chunks)} INFORMATION is done!')
    
    embeddings = np.array(embeddings)
    return embeddings
```

### Задание 4 - Загрузка основной информации о курсе в Qdrant

```python
def upload_info_to_qdrant(chunks: List[Dict], embeddings: np.ndarray, collection_name: str = 'karpov-guide-info'):
    """
    Загружает данные в Qdrant с оптимизированными параметрами.

    Эта функция загружает чанки основной информации о курсах и их эмбеддинги в коллекцию Qdrant. 
    Если коллекция уже существует, она будет удалена и создана заново.

    Параметры:
    - chunks (List[Dict]): Список чанков с основной информацией о курсе, включая описание и категории.
    - embeddings (np.ndarray): Эмбеддинги для чанков, полученные после векторизации текста.
    - collection_name (str): Название коллекции в Qdrant. По умолчанию 'karpov-guide-info'.

    Возвращаемое значение:
    - None
    """
    if qdrant_client.collection_exists(collection_name):
        qdrant_client.delete_collection(collection_name)  # Удаляем, если уже создана
            
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=..., distance=Distance.COSINE),
    )
    
    points = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        point = PointStruct(
            id=i,
            vector=embedding.tolist(),
            payload={
                'chunk_id': ...,
                'course_name': ...,
                'course_url': ...,
                'sequence': ...,
                'categories': ...,
                'metadata': ...
            }
        )
        points.append(point)
    
    qdrant_client.upload_points(
        collection_name=collection_name,
        points=points,
        batch_size=30
    )
```
